{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rethinking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVoQnZsiJN0bN1NmQjAuOR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo39P0GmqBrX",
        "outputId": "f66442fa-13c9-4088-87b8-c072411cdec7"
      },
      "source": [
        "# installing pyro\n",
        "!pip install pyro-ppl  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/7a/fbab572fd385154a0c07b0fa138683aa52e14603bb83d37b198e5f9269b1/pyro_ppl-1.6.0-py3-none-any.whl (634kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.8.1+cu101)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/81/957ae78e6398460a7230b0eb9b8f1cb954c5e913e868e48d89324c68cec7/pyro_api-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.0->pyro-ppl) (3.7.4.3)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfxkFqfRp5mE"
      },
      "source": [
        "import inspect\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributions import transform_to, constraints\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.ops.stats as stats\n",
        "import pyro.poutine as poutine\n",
        "from pyro.contrib.autoguide import AutoLaplaceApproximation\n",
        "from pyro.infer import TracePosterior, TracePredictive, Trace_ELBO\n",
        "from pyro.infer.mcmc import MCMC\n",
        "from pyro.ops.welford import WelfordCovariance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RWWsc9AqZvv"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)\n",
        "\n",
        "mp.set_sharing_strategy(\"file_system\")\n",
        "sns.set(font_scale=1.25, rc={\"figure.figsize\": (8, 6)})\n",
        "\n",
        "pyro.enable_validation()\n",
        "pyro.set_rng_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRXPt8GRqcJu"
      },
      "source": [
        "class MAP(TracePosterior):\n",
        "    def __init__(self, model, num_samples=10000, start={}):\n",
        "        super(MAP, self).__init__()\n",
        "        self.model = model\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def _traces(self, *args, **kwargs):\n",
        "        pyro.clear_param_store()\n",
        "\n",
        "        # find good initial trace\n",
        "        model_trace = poutine.trace(self.model).get_trace(*args, **kwargs)\n",
        "        best_log_prob = model_trace.log_prob_sum()\n",
        "        for i in range(10):\n",
        "            trace = poutine.trace(self.model).get_trace(*args, **kwargs)\n",
        "            log_prob = trace.log_prob_sum()\n",
        "            if log_prob > best_log_prob:\n",
        "                best_log_prob = log_prob\n",
        "                model_trace = trace\n",
        "\n",
        "        # lift model\n",
        "        model_trace = poutine.util.prune_subsample_sites(model_trace)\n",
        "        prior, unpacked = {}, {}\n",
        "        param_constraints = pyro.get_param_store().get_state()[\"constraints\"]\n",
        "        for name, node in model_trace.nodes.items():\n",
        "            if node[\"type\"] == \"param\":\n",
        "                if param_constraints[name] is constraints.positive:\n",
        "                    prior[name] = dist.HalfCauchy(200)\n",
        "                else:\n",
        "                    prior[name] = dist.Normal(0, 1000)\n",
        "                unpacked[name] = pyro.param(name).unconstrained().clone().detach()\n",
        "            elif name in self.start:\n",
        "                unpacked[name] = self.start[name]\n",
        "            elif node[\"type\"] == \"sample\" and not node[\"is_observed\"]:\n",
        "                unpacked[name] = transform_to(node[\"fn\"].support).inv(node[\"value\"])\n",
        "        lifted_model = poutine.lift(self.model, prior)\n",
        "\n",
        "        # define guide\n",
        "        packed = torch.cat([v.clone().detach().reshape(-1) for v in unpacked.values()])\n",
        "        pyro.param(\"auto_loc\", packed)\n",
        "        delta_guide = AutoLaplaceApproximation(lifted_model)\n",
        "\n",
        "        # train guide\n",
        "        loc_param = pyro.param(\"auto_loc\").unconstrained()\n",
        "        optimizer = torch.optim.LBFGS((loc_param,), lr=0.1, max_iter=500, tolerance_grad=1e-3)\n",
        "        loss_fn = Trace_ELBO().differentiable_loss\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(lifted_model, delta_guide, *args, **kwargs)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        guide = delta_guide.laplace_approximation(*args, **kwargs)\n",
        "\n",
        "        # get posterior\n",
        "        for i in range(self.num_samples):\n",
        "            guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
        "            model_poutine = poutine.trace(poutine.replay(lifted_model, trace=guide_trace))\n",
        "            yield model_poutine.get_trace(*args, **kwargs), 1.0\n",
        "\n",
        "    def run(self, *args, **kwargs):\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"error\")\n",
        "            for i in range(10):\n",
        "                try:\n",
        "                    return super(MAP, self).run(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    last_error = e\n",
        "        raise last_error\n",
        "\n",
        "\n",
        "def _formula_to_predictors(formula, data):\n",
        "    dtype = torch.get_default_dtype()\n",
        "    y_name, expr_str = formula.split(\" ~ \")\n",
        "    y_node = {\"name\": y_name, \"value\": torch.tensor(data[y_name], dtype=dtype)}\n",
        "    y_node[\"mean\"] = y_node[\"value\"].mean()\n",
        "\n",
        "    fit_intercept = True\n",
        "    predictors = {\"Intercept\": False}\n",
        "    col_to_num = dict(zip(data.columns, range(data.shape[1])))\n",
        "    expr_list = expr_str.split(\" + \")\n",
        "    for expr in expr_list:\n",
        "        if expr == \"0\":\n",
        "            fit_intercept = False\n",
        "        elif expr.startswith(\"I\"):\n",
        "            org_expr = expr\n",
        "            for col in col_to_num:\n",
        "                expr = expr.replace(col, \"c{}\".format(col_to_num[col]))\n",
        "            eval_expr = expr.lstrip(\"I\")\n",
        "            eval_map = {\"c{}\".format(i): data.iloc[:, i] for i in range(data.shape[1])}\n",
        "            predictors[org_expr] = torch.tensor(eval(eval_expr, eval_map), dtype=dtype)\n",
        "        elif expr.startswith(\"C\"):\n",
        "            cat_col = expr[2:-1]\n",
        "            for cat in data[cat_col].unique():\n",
        "                predictors[\"C(d){}\".format(cat)] = torch.tensor(data[cat_col] == cat, dtype=dtype)\n",
        "        elif expr in data.columns:\n",
        "            predictors[expr] = torch.tensor(data[expr], dtype=dtype)\n",
        "\n",
        "    if fit_intercept:\n",
        "        predictors[\"Intercept\"] = True\n",
        "    return y_node, predictors\n",
        "\n",
        "\n",
        "class LM(MAP):\n",
        "    def __init__(self, formula, data, num_samples=10000, start={}, centering=True):\n",
        "        self.formula = formula\n",
        "        self.y_node, self.predictors = _formula_to_predictors(formula, data)\n",
        "        self._predictor_means = {name: predictor.mean() for name, predictor\n",
        "                                 in self.predictors.items() if name != \"Intercept\"}\n",
        "        self.centering = centering\n",
        "        super(LM, self).__init__(self.model, num_samples, start)\n",
        "\n",
        "    def model(self, data=None):\n",
        "        if data is None:\n",
        "            y_node, predictors = self.y_node, self.predictors.copy()\n",
        "        else:\n",
        "            y_node, predictors = _formula_to_predictors(self.formula, data)\n",
        "        fit_intercept = predictors.pop(\"Intercept\")\n",
        "\n",
        "        mu = 0\n",
        "        if fit_intercept:\n",
        "            mu = mu + pyro.sample(\"Intercept\", dist.Normal(y_node[\"mean\"], 10))\n",
        "\n",
        "        for name, predictor in predictors.items():\n",
        "            coef = pyro.sample(name, dist.Normal(0, 10))\n",
        "            if fit_intercept and self.centering:\n",
        "                # use \"centering trick\"\n",
        "                predictor = predictor - self._predictor_means[name]\n",
        "            mu = mu + coef * predictor\n",
        "        sigma = pyro.sample(\"sigma\", dist.HalfCauchy(2))\n",
        "        with pyro.plate(\"plate\"):\n",
        "            return pyro.sample(y_node[\"name\"], dist.Normal(mu, sigma), obs=y_node[\"value\"])\n",
        "\n",
        "    def _get_centering_constant(self, coefs):\n",
        "        center = torch.tensor(0.)\n",
        "        for name, predictor_mean in self._predictor_means.items():\n",
        "            center = center + coefs[name] * predictor_mean\n",
        "        return center\n",
        "\n",
        "\n",
        "def glimmer(formula, data):\n",
        "    y_node, predictors = _formula_to_predictors(formula, data)\n",
        "    fit_intercept = predictors.pop(\"Intercept\")\n",
        "    print(\"def model({}):\".format(\", \".join(predictors.keys()) + \", {}\".format(y_node[\"name\"])))\n",
        "    mu_str = \"    mu = \"\n",
        "    if fit_intercept:\n",
        "        print(\"    intercept = pyro.sample('Intercept', dist.Normal(0, 10))\")\n",
        "        mu_str += \"intercept + \"\n",
        "    for predictor in predictors:\n",
        "        coef = predictor.replace(\"**\", \"_POW_\").replace(\"*\", \"_MUL_\").replace(\" \", \"\")\n",
        "        coef = re.sub(\"\\W\", \"_\", coef).strip(\"_\")\n",
        "        print(\"    b_{} = pyro.sample('{}', dist.Normal(0, 10))\".format(coef, predictor))\n",
        "        mu_str += \"b_{} * {}\".format(coef, predictor)\n",
        "    print(mu_str)\n",
        "    print(\"    sigma = pyro.sample('sigma', dist.HalfCauchy(2))\")\n",
        "    print(\"    with pyro.plate('plate'):\")\n",
        "    print(\"        return pyro.sample('{}', dist.Normal(mu, sigma), obs={})\"\n",
        "          .format(y_node[\"name\"], y_node[\"name\"]))\n",
        "\n",
        "\n",
        "def extract_samples(posterior):\n",
        "    nodes = poutine.util.prune_subsample_sites(posterior.exec_traces[0]).stochastic_nodes\n",
        "    node_supports = posterior.marginal(nodes).support(flatten=True)\n",
        "    return {latent: samples.detach() for latent, samples in node_supports.items()}\n",
        "\n",
        "\n",
        "def coef(posterior):\n",
        "    mean = {}\n",
        "    node_supports = extract_samples(posterior)\n",
        "    for node, support in node_supports.items():\n",
        "        mean[node] = support.mean(dim=0)\n",
        "    # correct `intercept` due to \"centering trick\"\n",
        "    if isinstance(posterior, LM) and \"Intercept\" in mean and posterior.centering:\n",
        "        center = posterior._get_centering_constant(mean)\n",
        "        mean[\"Intercept\"] = mean[\"Intercept\"] - center\n",
        "    return mean\n",
        "\n",
        "\n",
        "def vcov(posterior):\n",
        "    node_supports = extract_samples(posterior)\n",
        "    packed_support = torch.cat([support.reshape(support.size(0), -1)\n",
        "                                for support in node_supports.values()], dim=1)\n",
        "    cov_scheme = WelfordCovariance(diagonal=False)\n",
        "    for sample in packed_support:\n",
        "        cov_scheme.update(sample)\n",
        "    return cov_scheme.get_covariance(regularize=False)\n",
        "\n",
        "\n",
        "def precis(posterior, corr=False, digits=2):\n",
        "    if isinstance(posterior, TracePosterior):\n",
        "        node_supports = extract_samples(posterior)\n",
        "    else:\n",
        "        node_supports = posterior\n",
        "    df = pd.DataFrame(columns=[\"Mean\", \"StdDev\", \"|0.89\", \"0.89|\"])\n",
        "    for node, support in node_supports.items():\n",
        "        if support.dim() == 1:\n",
        "            hpdi = stats.hpdi(support, prob=0.89)\n",
        "            df.loc[node] = [support.mean().item(), support.std().item(),\n",
        "                            hpdi[0].item(), hpdi[1].item()]\n",
        "        else:\n",
        "            support = support.reshape(support.size(0), -1)\n",
        "            mean = support.mean(0)\n",
        "            std = support.std(0)\n",
        "            hpdi = stats.hpdi(support, prob=0.89)\n",
        "            for i in range(mean.size(0)):\n",
        "                df.loc[\"{}[{}]\".format(node, i)] = [mean[i].item(), std[i].item(),\n",
        "                                                    hpdi[0, i].item(), hpdi[1, i].item()]\n",
        "    # correct `intercept` due to \"centering trick\"\n",
        "    if isinstance(posterior, LM) and \"Intercept\" in df.index and posterior.centering:\n",
        "        center = posterior._get_centering_constant(df[\"Mean\"].to_dict()).item()\n",
        "        df.loc[\"Intercept\", [\"Mean\", \"|0.89\", \"0.89|\"]] -= center\n",
        "\n",
        "    if corr:\n",
        "        cov = vcov(posterior)\n",
        "        corr = cov / cov.diag().ger(cov.diag()).sqrt()\n",
        "        for i, node in enumerate(df.index):\n",
        "            df[node] = corr[:, i]\n",
        "\n",
        "    if isinstance(posterior, MCMC):\n",
        "        diagnostics = posterior.marginal(df.index.tolist()).diagnostics()\n",
        "        df = pd.concat([df, pd.DataFrame(diagnostics).T.astype(float)], axis=1)\n",
        "\n",
        "    return df.round(digits)\n",
        "\n",
        "\n",
        "def link(posterior, data=None, n=1000):\n",
        "    obs_node = posterior.exec_traces[0].observation_nodes[-1]\n",
        "    mu = []\n",
        "    if data is None:\n",
        "        for i in range(n):\n",
        "            idx = posterior._categorical.sample().item()\n",
        "            trace = posterior.exec_traces[idx]\n",
        "            mu.append(trace.nodes[obs_node][\"fn\"].mean)\n",
        "    else:\n",
        "        data = {name: data[name] if name in data else None\n",
        "                for name in inspect.signature(posterior.model).parameters}\n",
        "        predictive = TracePredictive(poutine.lift(posterior.model, dist.Normal(0, 1)),\n",
        "                                     posterior, n).run(**data)\n",
        "        for trace in predictive.exec_traces:\n",
        "            mu.append(trace.nodes[obs_node][\"fn\"].mean)\n",
        "    return torch.stack(mu).detach()\n",
        "\n",
        "\n",
        "def sim(posterior, data=None, n=1000):\n",
        "    obs_node = posterior.exec_traces[0].observation_nodes[-1]\n",
        "    obs = []\n",
        "    if data is None:\n",
        "        for i in range(n):\n",
        "            idx = posterior._categorical.sample().item()\n",
        "            trace = posterior.exec_traces[idx]\n",
        "            obs.append(trace.nodes[obs_node][\"fn\"].sample())\n",
        "    else:\n",
        "        data = {name: data[name] if name in data else None\n",
        "                for name in inspect.signature(posterior.model).parameters}\n",
        "        predictive = TracePredictive(poutine.lift(posterior.model, dist.Normal(0, 1)),\n",
        "                                     posterior, n).run(**data)\n",
        "        for trace in predictive.exec_traces:\n",
        "            obs.append(trace.nodes[obs_node][\"value\"])\n",
        "    return torch.stack(obs).detach()\n",
        "\n",
        "\n",
        "def compare(posteriors):\n",
        "    post_ics = {}\n",
        "    with torch.no_grad():\n",
        "        for name in posteriors:\n",
        "            post_ics[name] = posteriors[name].information_criterion(pointwise=True)\n",
        "    n_cases = post_ics[name][\"waic\"].size(0)\n",
        "    WAIC = {name: post_ics[name][\"waic\"].sum() for name in posteriors}\n",
        "    pWAIC = {name: post_ics[name][\"p_waic\"].sum() for name in posteriors}\n",
        "    SE = {name: (n_cases * post_ics[name][\"waic\"].var()).sqrt() for name in posteriors}\n",
        "    table = pd.DataFrame({\"WAIC\": WAIC, \"pWAIC\": pWAIC}).sort_values(by=\"WAIC\")\n",
        "    table[\"dWAIC\"] = table[\"WAIC\"] - table.iloc[0, 0]\n",
        "    table[\"weight\"] = torch.nn.functional.softmax(-1/2 * torch.tensor(table[\"dWAIC\"]), dim=0)\n",
        "    table[\"SE\"] = pd.Series(SE)\n",
        "    dSE = []\n",
        "    for i in range(table.shape[0]):\n",
        "        WAIC0 = post_ics[table.index[0]][\"waic\"]\n",
        "        WAICi = post_ics[table.index[i]][\"waic\"]\n",
        "        dSE.append((n_cases * (WAICi - WAIC0).var()).sqrt())\n",
        "    table[\"dSE\"] = dSE\n",
        "    return table.astype(float)\n",
        "\n",
        "\n",
        "def ensemble(posteriors, data):\n",
        "    weighted_num = (compare(posteriors)[\"weight\"] * 1000).astype(int)\n",
        "    weighted_num.iloc[-1] -= (sum(weighted_num) - 1000)\n",
        "    links = []\n",
        "    sims = []\n",
        "    for name in weighted_num.index:\n",
        "        num_samples = weighted_num[name]\n",
        "        links.append(link(posteriors[name], data, num_samples).reshape(num_samples, -1))\n",
        "        sims.append(sim(posteriors[name], data, num_samples).reshape(num_samples, -1))\n",
        "    num_data = max(l.size(1) for l in links)\n",
        "    links = [l.expand(-1, num_data) for l in links]\n",
        "    sims = [s.expand(-1, num_data) for s in sims]\n",
        "    return {\"link\": torch.cat(links), \"sim\": torch.cat(sims)}\n",
        "\n",
        "\n",
        "def _worker(n, fn, fn_args, child_info=None):\n",
        "    if child_info is not None:\n",
        "        idx, event, queue = child_info\n",
        "        pyro.set_rng_seed(idx)\n",
        "    result = []\n",
        "    for i in range(n):\n",
        "        item = fn(*fn_args)\n",
        "        result.append(item)\n",
        "        queue.put((idx, item))\n",
        "        event.wait()\n",
        "        event.clear()\n",
        "    return result\n",
        "\n",
        "\n",
        "def replicate(n, fn, fn_args, mc_cores=None):\n",
        "    mc_cores = mp.cpu_count() - 1 if mc_cores is None else mc_cores\n",
        "    queue = mp.Queue()\n",
        "    events = [mp.Event() for i in range(mc_cores)]\n",
        "    processes = []\n",
        "    for i in range(mc_cores):\n",
        "        n_i = n // mc_cores + (i < n % mc_cores)\n",
        "        child_info = (i, events[i], queue)\n",
        "        p = mp.Process(target=_worker, args=(n_i, fn, fn_args, child_info), daemon=True)\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    result = []\n",
        "    for i in range(n):\n",
        "        idx, item = queue.get()\n",
        "        result.append(item)\n",
        "        events[idx].set()\n",
        "\n",
        "    for i in range(mc_cores):\n",
        "        processes[i].join()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LsplDuZxCT9"
      },
      "source": [
        "## TensorFlow Probability\n",
        "\n",
        "Define the model and compute the posterior.\n",
        "Define the model by tfd.JointDistributionCoroutine()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8lS5V08xU1n"
      },
      "source": [
        "Here is the description of the linear regression model that use Median Age as the predictor variable.\n",
        "\n",
        "$D_i \\sim Normal(\\mu_i,\\sigma)$    \n",
        "\n",
        "$\\mu_i = \\alpha + \\beta_AA_i$   \n",
        "\n",
        "$\\alpha \\sim Normal(0,0.2)$       \n",
        "\n",
        "$\\beta_A \\sim Normal(0,0.5)$          \n",
        "\n",
        "$\\sigma \\sim Exponential(1)$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thcnO0rJxJsR"
      },
      "source": [
        "tdf = df_to_tensors('WaffleDivorce', d, ['A', 'D'])\n",
        "\n",
        "def model_5_1(median_age_data):\n",
        "    def _generator():\n",
        "      alpha = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.2, name=\"alpha\"), sample_shape=1))\n",
        "      betaA = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.5, name=\"betaA\"), sample_shape=1))\n",
        "      sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1., name=\"sigma\"), sample_shape=1))\n",
        "    \n",
        "      mu =  alpha + betaA * median_age_data\n",
        "        \n",
        "      divorce = yield tfd.Independent(tfd.Normal(loc=mu, scale=sigma, name=\"divorce\"), reinterpreted_batch_ndims=1)\n",
        "\n",
        "    return tfd.JointDistributionCoroutine(_generator, validate_args=True)    \n",
        "    \n",
        "jdc_5_1 = model_5_1(tdf.A)   # d[\"A\"] = d.MedianAgeMarriage.pipe(lambda x: (x - x.mean()) / x.std())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}